# -*- coding: utf-8 -*-
"""Rag_using_pinecone.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JpQ7Vv3eOp5G2HWEWc7UwAPCGUXbHGLH
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install --upgrade --quiet  pinecone pinecone-text pinecone-notebooks

# Connect to Pinecone and get an API key.
# from pinecone_notebooks.colab import Authenticate

# Authenticate()

# import os

api_key = os.environ["PINECONE_API_KEY"]

print(api_key)

# !pip install langchain-community

from langchain_community.retrievers import (
    PineconeHybridSearchRetriever,
)

import os

from pinecone import Pinecone, ServerlessSpec

index_name = "langchain-pinecone-hybrid-search"

# initialize the Pinecone client
pc = Pinecone(
    api_key=api_key)

#create the index
if index_name not in pc.list_indexes():
    pc.create_index(
        name=index_name,
        dimension=384, #dimension of densed vector
        metric="dotproduct", ## sparse values supported only for dotproduct
        spec = ServerlessSpec(cloud="aws", region="us-east-1")
    )

index = pc.Index(index_name)
index

# !pip install langchain-huggingface

## vector embedding and sparce matrix
import os
from dotenv import load_dotenv
load_dotenv()

os.environ["HUGGING_FACE_HUB_TOKEN"] = ""

from langchain_huggingface import HuggingFaceEmbeddings
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
embeddings

from pinecone_text.sparse import BM25Encoder # uses tfidf

bm25_encoder = BM25Encoder().default()
bm25_encoder

import nltk

nltk.download('punkt_tab')

sentences = [
    "The sun sets behind the mountains, painting the sky in shades of orange and purple.",
    "Hybrid search combines keyword and semantic methods to improve search accuracy.",
    "A gentle breeze rustled the leaves, bringing a refreshing coolness to the warm afternoon."
]

# apply tfidf values on these sentence
bm25_encoder.fit(sentences)

# store the values to a json file
bm25_encoder.dump("bm25_values.json")

# load to your BM25Encoder object
bm25_encoder = BM25Encoder().load("bm25_values.json")

retriever = PineconeHybridSearchRetriever(
    embeddings=embeddings,
    sparse_encoder=bm25_encoder,
    index=index)

retriever

retriever.add_texts(
    [
        "The orange and purple.sun sets behind the mountains, painting the sky in shades of ",
        "Hybrid search combines keyword and semantic methods to improve search accuracy.",
        "A gentle breeze rustled the leaves, bringing a refreshing coolness to the warm afternoon."
    ]
)

retriever.invoke("What is the method to improve search accuracy?")

from huggingface_hub import HfApi

api = HfApi()
try:
    api.model_info("google/flan-t5-base", token="")  # Replace with your token
    print("Token is valid and has access to the model.")
except Exception as e:
    print(f"Error: {e}")

from langchain.chains import RetrievalQA
from langchain_huggingface import HuggingFaceEndpoint
from langchain.prompts import PromptTemplate


os.environ["HUGGING_FACE_HUB_TOKEN"] = ""  # Use your token

# Load the flan-t5-base model
llm = HuggingFaceEndpoint(
    repo_id="google/flan-t5-base",
    huggingfacehub_api_token=os.environ["HUGGING_FACE_HUB_TOKEN"],
    task="text-generation"
)

print("Model loaded successfully!")

qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="map_reduce",
    retriever=retriever,
    return_source_documents=True
)


query = "what shades is the color of the sky painted off ?"
response = qa_chain({"query": query})

print("Answer:", response["result"])
print("Source Documents:", response["source_documents"])

qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="map_reduce",
    retriever=retriever,
    return_source_documents=True
)

print("Welcome to the RAG Chatbot! Ask a question about the sky, search accuracy, or anything related to the documents. Type 'quit' or 'exit' to end the chat.")

while True:
    # Get user input
    query = input("\nYour question: ").strip().lower()

    # Check for exit condition
    if query in ['quit', 'exit']:
        print("Goodbye! Thanks for chatting.")
        break

    try:
        response = qa_chain({"query": query})
        print("Answer:", response["result"])
        print("Source Documents:", response["source_documents"])
    except Exception as e:
        print(f"Error processing your query: {e}")
        print("Please try again with a different question or check your setup.")

print("Chatbot session ended.")

